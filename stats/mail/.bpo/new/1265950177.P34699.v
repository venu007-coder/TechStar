Return-Path: surendervelusamyy@gmail.com
Received: from mx93.stngva01.us.mxservers.net (198.173.112.10)
	by mail19k.g19.rapidsite.net (RS ver 1.0.95vs) with SMTP id 2-06787177
	for <bpojobs@altechstar.com>; Thu, 11 Feb 2010 23:49:37 -0500 (EST)
Received: from unknown [209.235.156.156] (EHLO mail16c40.carrierzone.com)
	by va1-mx93.stngva01.us.mxservers.net (mxl_mta-3.1.0-05)
	with ESMTP id 0edd47b4.2585758624.590270.00-007.va1-mx93.stngva01.us.mxservers.net (envelope-from <surendervelusammy@gmail.com>);
	Thu, 11 Feb 2010 23:49:36 -0500 (EST)
X-Authenticated-User: kishore.softwaretechnocratz.com
Received: from GOV ([59.93.82.27])
	(authenticated bits=0)
	by mail16c40.carrierzone.com (8.13.6/8.13.1) with ESMTP id o1C4ndMV004702
	(version=TLSv1/SSLv3 cipher=RC4-MD5 bits=128 verify=NO)
	for <bpojobs@altechstar.com>; Thu, 11 Feb 2010 23:49:44 -0500
Reply-To: surendervelusamyy@gmail.com
Message-ID: <ba36441d0e31daf7f89ef05c001dbe1b@gmail.com>
From: "Surender Vel" <surendervelusammy@gmail.com>
To: <bpojobs@altechstar.com>
Subject: =?windows-1252?Q?Resume_of_Senior_ETL_Lead_Consultant_(Informatica,_Microsoft_SSIS)_looking_for_contract_assignments.?=
Date: Thu, 11 Feb 2010 23:38:30 -0500
MIME-Version: 1.0
Content-Type: text/plain;
	charset="windows-1252"
Content-Transfer-Encoding: quoted-printable
X-Spam: [F=0.2000000000; B=0.500(0); S=0.200(2010011101); MH=0.500(2010021141)]
X-MAIL-FROM: <surendervelusammy@gmail.com>
X-SOURCE-IP: [209.235.156.156]
X-SF-Loop: 1

Hi,

I am a Senior ETL Lead Consultant with over Seven years of experience in =
Information Technology (IT) industry with specialization in the Data =
Analysis, Design, Modeling, Development, Testing, Implementation and =
Maintenance of  OLTP/Data Warehousing/OLAP Reporting Applications=2E
=95	Extensive experience in overall design, development, and =
implementation of the companies data warehouse applications, data marts, =
ETL processes, extracts and associated interfaces for medium to large =
projects using Informatica, Hyperion Essbase, Business Objects, SSAS etc

Domain Knowledge:  Financial , Health Care sector,  Manufacturing, =
Insurance.=20

Clients Worked for:
Health Management Corporation, Richmond VA=09
JP Morgan Chase, Wilmington DE
Qimonda AG, Richmond VA
Genworth Financials, Richmond VA
Tata Consultancy Services

Certifications=20
=95	Business Objects Designer and Reporter Module (TCS BO CoE)
=95	NSE Certification in Financial Markets (NCFM)

Work Status:        H1B visa=2E
Position type:       Corp to Corp contracts only=2E
Current Location:  Richmond,VA=2E
Relocation:           I am ready to relocate anywhere in US and will =
attend face to face interviews on my own=2E

If you have a suitable contract, please send me details=2E
I will call you back once i receive job details=2E

Thanks,

Surender

                                  =20
                                                                           =
                                                         Resume
                                                                           =
                      	 	                  Surender V
                                                                           =
                                          surendervelusamyy@gmail.com

=95	Senior ETL Lead Consultant with over Seven years of experience in =
Information Technology (IT) industry with specialization in the Data =
Analysis, Design, Modeling, Development, Testing, Implementation and=20
              Maintenance of  OLTP/Data Warehousing/OLAP Reporting =
Applications
=95	Assumed various roles including Lead Developer/Data Analyst/ ETL =
Analyst in Financial , Health Care sector,  Manufacturing, Insurance =
sector for 6 years with strong domain knowledge=20
=95	Extensive experience in overall design, development, and =
implementation of the companies data warehouse applications, data marts, =
ETL processes, extracts and associated interfaces for medium to large=20
              projects using Informatica, Hyperion Essbase, Business =
Objects, SSAS etc
=95	Expert level knowledge in Very Large Oracle Databases with full =
understanding of performance implications based on the Hardware and =
Application Design.=20
=95	Extensive experience in database design for data warehouses with =
strong understanding of Dimensional Modeling, Star and Snowflake Schemas =
using various OLAP modeling techniques using Erwin 4.0 and=20
              MS Visio
=95	Extensively worked in Oracle 10g/9i/8i and SQL Server 7.x/2000/2005 =
(External Tables, PL/SQL Procedures, Functions, Packages, Triggers , Job =
Scheduler and SQL/T-SQL)   databases to transfer data=20
=95	Expertise in Performance Tuning (Database Tuning, SQL Tuning, =
Application/ETL Tuning) and DBA activities =96 DB setup, Managing User =
Roles and Profiles, Partition maintenance and Statistic Maintenance
=95	Experience working in Informatica Power Center 8.x/7.x/6.x/5.x =
Mappings, Mapplets, Transformations, Workflow Manager, Workflow Monitor, =
Repository Manager
=95	Experience in developing ETL Packages, Stored Procedures, Views, =
Functions & Triggers, T-SQL (Transact-SQL) using SQL Server Integration =
Services (SSIS)
=95	Extensive experience in Business Objects XI/6.x/5.x Universe Designer. =
Desktop Intelligence, Supervisor,  Central Management Console, Import =
Wizard, Info View, Web Intelligence Explorer, Web Intelligence=20
              Reporter, ZABO
=95	Working Knowledge in Teradata. Used loading Utilities like Fast Load, =
MultiLoad, TPUMP, and Fast Export. Proficiency in Teradata SQL queries
=95	Experience in designing and developing  multidimensional OLAP Cubes =
and Dimensions using SQL Services Analysis Services (SSAS)
=95	Experience working with Crystal Reports 2008=20
=95	Experience in design, analysis, development and implementation of =
various applications in Client/Server environment using Hyperion Essbase =
9.X/7.X/6.X, Spreadsheet Services, under Windows 2000/XP and=20
              Unix Environment
=95	Experience in scheduling jobs in various environment using Appworx, =
Cron and Oracle Job Scheduler
=95	Extensive experience in using and administration of File Transfer =
tools Connect: Direct and FTP/SCP. Experience in GPG Encryption/Decryption =
for secured file transmission
=95	Extremely articulate in verbal / written communication with an =
exceptional character for customer service with experience in coordinating =
various client meetings, seminars, presentations and group=20
             discussions and facilitating the organizations to bridge the =
gap between business people and the data
=95	Dedicated employee and dependable team player and leader, exceedingly =
detail oriented and an expeditious learner, creative problem solver and =
analytically inclined

Technical Skills
Business Areas:	   Financial/Finance, Insurance, Manufacturing and =
Inventory Management
System Aspects:	   Data Modeling (ER & Dimensional), Database Design, =
Requirement Analysis, Business Rule Implementation, ETL Design, ETL =
Architecture, Object Oriented Design, Development, Testing,=20
                               Performance Tuning, Data Mapping, Metadata =
Management, Data Profiling, Data Stewardship, Data Certification and =
Publishing, Deployment, Documentation, 24x7 Production Support,
                               Project Management
Modeling Tools: 	   CA Erwin 4.0, Platinum Erwin, MS Visio
RDBMS: 	                 Oracle 10g/9i/8i, SQL Server 7.0/2000/2005/2008, =
MS Access, Teradata
ETL tools: 	   Informatica Power Center 8.x/7.x/6.x, Microsoft SQL Server =
Integration Services (SSIS)
Reporting Tools: 	   Business Objects XI/6.x/5.x, Crystal Reports 2008, =
Oracle Application Express
OLAP Tools:	   Essbase 9.1, Hyperion Financial Management, Microsoft SQL =
Server Analysis Services (SSAS), Excel Essbase Add-in, Active OLAP, Dodeca
Programming: 	   C, SQL, T-SQL, PL/SQL, B-TEQ, Teradata SQL
Operating systems:    Windows NT/95/98/2000, DOS, UNIX (HP, SUN Solaris, =
AIX), LINUX
Scripting languages:   UNIX Shell (K-shell/C-shell), AWK, Perl, JavaScript
Web technologies: 	    HTML, DHTML, XML
Tools/Utilities: 	    PL/SQL Developer, Toad 7.x, SQL Navigator, Oracle =
Developer, SQL* PLUS, Mercury Test Director and IT Work Management, SQL =
Server Management Studio,
                                SQL Server Business Intelligent Studio, =
Teradata SQL Assistant
Job Schedulers: 	    Appworx 5.1, UNIX Cron, Oracle Job Scheduler, TIDAL

Education=20
=95	Bachelor of Engineering (Mechanical Engineering)

Certifications=20
=95	Business Objects Designer and Reporter Module (TCS BO CoE)
=95	NSE Certification in Financial Markets (NCFM)

Project Profiles

Integrated Health Model (IHM) Operational Reporting Project=20
Health Management Corporation, Richmond VA				Oct 2009 =96 Till Date

ETL Consultant:
The Medical Management Utilization Management (UM) data is a critical =
component of the Integrated Health Model (IHM) Operational Reporting =
Project as it one of the two foundational data sources. During =
requirements definition, it was determined that rather than bypassing the =
InfoHub as originally planned, efforts could be consolidated by directly =
loading the needed WMDS extract to the InfoHub directly.  This would =
create the process by which to load WMDS Medical Management data into the =
InfoHub for. This would also satisfy the data source requirements for IHM. =
The InfoHub will serve as a data repository which will enable HMC to =
appropriately identify and stratify WellPoint Medical Decision Support =
(WMDS) and in the future Empire Condition Care (ECC) members

Role:
=95	Designed and developed time-bound Informatica processes for generating =
and transmitting extracts to various out-bound interfaces
=95	Created new mappings/workflows in Informatica Power Center to load =
data from heterogeneous data sources=20
=95	Extensively used almost all of the transformations of Informatica =
including lookups, Stored Procedures, Update Strategy and others
=95	Incorporated the Informatica Velocity approach for Mapping design, =
development and Data Governance
=95	Worked as a Data Steward in requirement phase to create Source to =
Target Mapping document for Incoming data into the Enterprise DWH. This =
involves Data Profiling of the Source data and defining=20
              Organization Standards
=95	Worked closely with Data Modeler to build a robust model to provide =
Scalability to the Target tables in the DWH
=95	Designed and developed reusable Error Handling and Backout process =
that can be reused across organization
=95	Worked on Informatica Versioning. Performed Informatica Admin tasks of =
Labeling and assigning Deployment groups with the Correct Version=2E
=95	Created Stored Programs and Functions to achieve better performance of =
the data loading and backout process
=95	Implemented data load control process to prevent multiple loads for a =
process at the same time
=95	Involved actively in the Migration of data from Development to System =
Testing to Production environment
=95	Extensively used Mapping Parameters and Variables, Session Parameters =
and Parameter files to automatic daily loading process
=95	Created SSIS packages to load the data from Information Hub and =
Healthy Returns System database into IHM Data Mart
=95	Create database maintenance plans for the performance of SQL Server =
including database integrity checks, update database statistics, =
re-indexing and data backups=20
=95	Designed and developed Adhoc, canned and parameterized reports using =
Crystal reports 2008
=95	Used Business Objects Central Management Console to maintain Folders =
and User access for reports
=95	Designed and developed various kinds of reports like Long, Medium and =
Short versions for all the divisions and entities
Environment: Informatica Power Center 8.6.1, SQL Server 2005/2008, SQL =
Server Integration Services 2005/2008, Tidal Scheduler, SSIS, Crystal =
Reports 2008, Windows NT Server and Custom build Reporting applications

CPB Finance MIS Project
JP Morgan Chase, Wilmington DE			     	            	Mar 2009 =96 Oct 2009

DWH Consultant:
This project involves automation and maintenance of Chase Profit and Loss =
data loading process from heterogeneous sources into Account level =
profitability data mart and Common Profit Book (CPB) Essbase cubes that =
provides portfolio revenue and expense participation at the account level =
to support analysis across segments, behaviors, FICO/risk, and product =
types. This project also involves enhancement and maintenance of GLFAAS =
(General Ledger Forecasting and Analysis System), DKS (Daily Key =
Statistics), STRAT (Balance Stratification), Revshare (Partner level =
Revenue Sharing), Plastics and Customer Dashboard Cubes on a regular =
basis. It also involves integration of new portfolios into existing Data =
mart and decommissioning of old portfolios.=20

Role:
=95	Responsible for creating CPB Finance monthly production data from ADS, =
ODS and external data sources into CPB Data Mart in UNIX/Oracle environment
=95	Actively involved in the design and development of WAMU Data Migration =
and integrating into existing CPB cube
=95	Designed and developed various kinds of reports like Long, Medium and =
Short versions for all the divisions and entities
=95	Created new mappings/workflows in Informatica Power Center to load =
data from heterogeneous data sources=20
=95	Designed and developed time-bound Informatica processes for generating =
and transmitting extracts to various out-bound interfaces
=95	Extensively used almost all of the transformations of Informatica =
including lookups, Stored Procedures, Update Strategy and others
=95	Created Generic PL/SQL packages, procedures , functions, triggers and =
SQL scripts to automate the daily/weekly/monthly loading process
=95	Created reports using Hyperion Financial Reports and Smartview (Excel =
Add-In) from HFM (Hyperion Financial Management)
=95	Creating Block Storage Cubes, Outlines, Load Rules and CALC scripts to =
automatically load the allocated monthly  finance data for all the =
portfolio=92s into Essbase Cubes
=95	Converted existing Block Storage (BSO) Cubes to Aggregate Storage =
(ASO) cubes to improve Cube performance
=95	Experienced in writing custom Essbase calculation scripts and =
automating the Essbase cubes using MaxL, ESSCMD and Batch/Shell Scripts
=95	Experience working as a Essbase administrator using Hyperion Admin =
Console (7x, 9x) to control the data loading and security of the cubes
=95	Interface with finance group on a daily basis to fix production and =
post production errors or amendments and Adhoc report requests
Environment: Informatica Power Center 8.x, Oracle, PL/SQL, SQL, Pro C, =
Unix Shell, AWK, PERL, Hyperion Essbase System 9, Hyperion Financial =
Management System 9, Essbase Administration Services, Essbase Excel =
Smartview/Add-In, HP UNIX, Windows NT Server and Custom build Reporting =
applications

200mm and 300mm DWH Project
Qimonda AG, Richmond VA			     		            Nov 2007 =96 Feb 2009

DWH Consultant:
This project involves maintaining and enhancing the existing 200mm and =
300mm DWH for North America division. It also involves aligning the local =
DWH to Global DWH (4-tiered architecture) with local and global reporting =
capabilities=2E

Role:
=95	Architecting Work Flows, Activity Hierarchy & Process Flows; =
Documenting using Interface Diagrams, Flow Charts & Specification Documents
=95	Setup the Informatica Power Center, Database and UNIX environment for =
the project and performed the role of an Administrator for setting up the =
development/staging/production environment for the=20
              project=20
=95	System Retirement - Retiring old legacy systems after reengineering; =
Integration with ODS. Maintaining  and  Enhancing the existing DWH Real =
Time Data Replication process from MES and other data=20
              sources using Informatica
=95	Coded UNIX Shell scripts to convert files into a format compatible to =
Informatica scripts=2E
=95	Designed and developed 200mm and 300mm business Data Marts  and =
calculated Key Performance Indicators ( IN, OUT, VOL, WIP, YLD, CT etc..) =
per  time, technology, facility and standardized reports to=20
              control and optimize the production FAB of Qimonda AG
=95	Extensively worked in Oracle External Tables, PL/SQL Procedures, =
Functions, Packages, Triggers and Oracle Job Scheduler to transfer the =
data from DWH to downstream reporting applications
=95	Created DTS/SSIS packages for Uploading of Various different format of =
files and databases to MS SQL Server using Data Transformation Services =
(DTS) and Integration Services (SSIS)
=95	Maintained and upgraded the custom build Multi Dimensional reporting =
tool MARWIN  using Microsoft SQL Server Analysis Services=20
=95	Enhanced the existing web based application and developed ad hoc =
reports  using Oracle Application Express (formerly called HTML DB)  that =
will manage and retrieve the data directly from the ODS and=20
              DWH
=95	Worked on all the modules in Business Objects XI like Designer, =
Desktop Intelligence, Import Wizard, Report conversion Tool and =
Application Foundation (Dashboard Manager & Performance Manager)
Environment: Oracle 10g/9i, SQL Server 2000/2005, Informatica Power Center =
8.x, Oracle Application Express 2.0/3.0, Business Objects XI, Java/JSP, MS =
Visio, MS Access, HP UNIX, Windows NT Server and Custom build Reporting =
applications

Genworth Financials, Richmond VA			     		Sep 2006 =96 Oct 2007
Group LTC Data warehousing Project

Project Lead:
Genworth Financial Assurance (GNW) has introduced a Group Long Term Care =
product. The Group Business is fulfilled at LTCG (TPA). This creates gaps =
in the information available in our current business systems (SMART, POLY, =
and CALYPSO) that are required for us to do business. This project is =
undertaken to create a Group ODS to store the transactional data of Group =
LTC business administered by LTCG and build a Central Group DWH with Adhoc =
reporting capabilities=20

Role:
=95	Worked with a team of 10 offshore developers and 3 onsite team members =
providing hands on guidance to team members to convert Business =
Requirements into deliverables=2E
=95	Played strategic decisive role as part of core technical architecture =
team to develop a Group LTC Data Warehouse from the Third Party =
Administrator=92s (TPA) Legacy system flat files
=95	Logical & Physical Data Modeling of Group LTC Data warehouse using =
Erwin, which involved developing Entity Relationship Diagrams (ERD) and  =
implementation of Business Rules
=95	Metadata & Data Dictionary Management; Data Profiling; Data Mapping. =
ETL Design & Implementation - Data Extraction, Transformation & Loading =
(using Informatica & SQL Loader)
=95	Created Informatica mappings/workflows in Power Center to load the =
transactional ODS and Group LTC reporting data warehouse and additional =
mappings/workflows to load summarized tables for better=20
              reporting performance=2E
=95	Performance tuned Oracle SQL queries by using query plan and creating =
indexes to minimize query execution times
=95	Created UNIX shell scripts AWK scripts to convert files into a format =
compatible to Informatica
=95	Created a Group Universe using the Designer Component of Business =
Objects with Adhoc reporting capabilities
Environment: Informatica Power Center 8.1/7.1, Business Objects 6.5, =
Oracle 9i/10g, Erwin, MS Visio, SQL, Sun Solaris OS 2.9, Windows NT Server

Medicare Supplement Project
Genworth Financials, Richmond VA			         		Jan 2005 =96 Aug 2006

ETL Lead:
The Medicare Supplement Reporting project is to develop a Data Warehouse =
to support a reporting environment for Genworth Medicare Supplement =
business and provide necessary information regarding the Medicare =
Supplement product to various LTC functions, which includes Finance, =
Actuarial, Compliance, Risk, Operations, Finance, Product Marketing & =
Distribution, and Sales. Genworth acquired Continental Life Assurance in =
2005. A new project is undertaken to create a Medsupp ODS to load all the =
Continental data. This project also involves creating an integrated Data =
mart that enables the business to access both Continental and WAKELY business

Role:
=95	Analyzing source systems and feasibility study for Medsupp Data =
Integration of existing WAKELY and new Continental (CLI and ACI) data into =
a Enterprise Medsupp Data warehouse
=95	Worked with Middleware layers and Enterprise data architecture strategy
=95	Designed and developed Medicare Operational Data Store using Erwin =
data modeler for loading all the CLI data into Medsupp data warehouse=2E
=95	Proficiently apply appropriate methodologies for logical and physical =
database design towards creating a user friendly Ad hoc system
=95	Coordinating with DBA team to implement physical models & to setup =
development, test, staging & production environments using Erwin automated =
script macros
=95	Extensively worked with Informatica Workflow Manager, Workflow =
Monitor, Designer and Repository Manager, Source Analyzer and Warehouse =
Designer
=95	Created Batch jobs with GPG encryption/decryption capabilities for =
secured file transmission and scheduling of all the jobs in various =
environment using Appworx
=95	OLAP Reporting semantic layer (universe) design using Business Objects =
Designer. Created Business Objects OLAP reports with slice-and-dice =
capabilities in Web Intelligence /ZABO client
=95	Worked closely with the team members to make sure that implementations =
went as per the requirements by providing technical level feedback as well =
as functional testing feedback
Environment: Oracle 10g/9i, SQL Server 2000, SQL, Business Objects 6.5, =
Crystal Reports 10, Informatica Power Center 8.1/7.1, Sun Solaris OS 2.9, =
Windows NT Server

Employee Payroll Feeds (LTC Group)
Genworth Financials, Richmond VA		         			Aug 2005 =96 Feb 2006

ETL Development Lead:
Genworth Financial Assurance (GNW) has introduced a Group Long Term Care =
product. The product will be sold to qualifying companies. As a function =
of administering the product, LTCG will be responsible for billing. LTCG =
does not have the multi payer option; this project is being undertaken by =
Genworth to provide this functionality

Role:
=95	Interaction with end-users and business analysts to identify and =
develop business requirements and transform it into technical requirements =
and created Functional and Technical Design documents
=95	Extracted, scrubbed and transformed data from Flat Files and Oracle. =
Loaded staging tables using Informatica=2E
=95	Extensively used almost all of the transformations of Informatica =
including lookups, Stored Procedures, Update Strategy and others
=95	Worked with Memory cache for static and dynamic cache for the better =
throughput of sessions containing Rank, Lookup, Joiner, Sorter and =
Aggregator transformations and created Parameters and variables=20
              that are  used in many different transformations for =
creating a Mapping=2E
=95	Created various Reusable and Non-Reusable tasks like Session, =
Mapplets, Assignment, Command, Control, Decision, Event and Timer and used =
Pre and Post session SQL and Unix Commands=2E
=95	Coded Shell scripts to convert files into a format compatible to =
Informatica scripts. Extensively used GPG encryption/decryption for =
secured file transmission
=95	Performance tuned Oracle SQL queries by using query plan and creating =
indexes to quicken query execution times=20
=95	Preparing Unit Test Plans (UTPs) for testing the changes for the =
business requests and production move
Environment: Informatica Power Center 7.1, Oracle 9i, SQL, Sun Solaris OS =
2.9, Erwin Data Modeler, Test Director, Windows NT Server

LTC Credit Card System (NBES)
Tata Consultancy Services Mumbai, India			            Dec 2004 =96 Jul 2005

ETL/Business Objects Developer
=95	Involved in installing the Business Objects, creating Repository, =
creating Users and User Groups using supervisor models=2E
=95	Created a LTC Credit Card System Business Objects Universe using the =
Designer Component of Business Objects
=95	Generated Business Objects reports using Universes as the main data =
providers and writing the complex queries including Sub queries, Unions, =
Intersect and Aliases
=95	Scheduling of Reports using Broadcast Manager=20
=95	Developed Hierarchies to support drill down reports.=20
=95	Involved in Creating Users, User Groups and granting them permissions =
to access the reports.=20
=95	Tuned Universes & Reports for better performance.=20
=95	Extensively worked on Oracle in day to day activities=2E
=95	Designed and Developed ETL process using Informatica Power Center
Environment: Business Objects 5.1, Informatica Power Center 6.2, Oracle =
9i, Sun Solaris OS 2.9

Application Redesign Project
Tata Consultancy Services Mumbai, India			         	Apr 2004 =96 Dec 2004

Asst. System Analyst:
=95	Requirement Analysis and creating a detailed Technical and Functional =
Design document and weekly meetings with the Client and Status reporting
=95	Managed data massaging requirements in an Enterprise Application =
Integration environment; Worked with Middleware layers and Enterprise data =
architecture strategy
=95	Developing PL/SQL packages, workflows and database triggers
=95	Performance tuned Oracle SQL queries
=95	Have done knowledge transfer and preparing knowledge transfer =
documents and involved in creating detailed design documents and involved =
in walkthrough meetings with clients
Environment: Informatica 5.1, Oracle 9i, PL/SQL, HTML, JSP, Sun Solaris OS =
2.9, SQL Loader, Explain Plan, TKPROF=20

Marketing Experience Project					               Apr 2003 =96 Mar 2004=20

IT Developer - Trainee:
=95	Resolution of day to day loading issues and technical challenges in =
the Project. Impact Analysis and effort estimation of Change requests.=20
=95	Daily Client interaction and interaction with other teams. Solving =
various user problems and providing efficient and timely solutions to those
=95	PL/SQL code review - Stored Procedures, Functions & Triggers using =
PL/SQL. Unix Shell Scripting for automation of various processes and new =
enhancements.=20
=95	Troubleshooting Informatica Mapping, Workflow issues and Scheduling =
and maintaining UNIX Cron entries for automated loading of data
Environment: Informatica Power Mart 5.1, Oracle 8.1.7, SQL Loader, Solaris =
2.6, Windows 2000, IT Work Management for change requests, Data Clean and =
CoRS


